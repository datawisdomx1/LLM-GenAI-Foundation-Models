{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c252025e-d4f7-4225-9a26-fd4a2d40621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00439e70-9258-4fbd-ba32-2a33eb6fe1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "# model_inputs = encodeds.to(device)\n",
    "# model.to(device)\n",
    "\n",
    "# generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "# decoded = tokenizer.batch_decode(generated_ids)\n",
    "# print(decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca67090-cf45-40fd-911b-fb02fdcf6d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U bitsandbytes transformers peft accelerate trl datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe7810e-d506-4e0e-943e-79f3766857f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf0faac-bfda-417e-b858-753be4a0ff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b68f220-4708-4aab-8fbf-0ec4ee1a4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch \n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f65123-7734-4657-bb87-62ed79947e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "# secret_wandb = user_secrets.get_secret(\"wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa55a11-acc2-48f9-9b9c-33576c39349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!huggingface-cli login --token $secret_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7afd1a48-3dc4-4eec-b6f2-9ea210b7bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login(key = secret_wandb)\n",
    "# run = wandb.init(\n",
    "#     project='Fine tuning mistral 7B', \n",
    "#     job_type=\"training\", \n",
    "#     anonymous=\"allow\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ca1d23-e7ce-4440-b815-b14ca64d81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
    "# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# new_model = \"mistral_7b_guanaco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa59b3a-bf73-4d44-b68c-3c6265408ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model from Hugging Face hub\n",
    "# base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# # New instruction dataset\n",
    "# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# # Fine-tuned model\n",
    "# new_model = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "241ca91f-20b7-4e64-830a-32af8e1c696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# New instruction dataset\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"llama-2-7b-chat-guanaco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd72331e-17e8-46b7-8ae4-4c954be135cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] cuanto es 2x2 xD [/INST] La respuesta es 4. </s><s>[INST] puedes demostrarme matematicamente que 2x2 es 4? [/INST] En una multiplicación, el producto es el resultado de sumar un factor tantas veces como indique el otro, es decir, si tenemos una operación v · n = x, entonces x será igual a v sumado n veces o n sumado v veces, por ejemplo, para la multiplicación 3 · 4 podemos sumar \"3 + 3 + 3 + 3\" o \"4 + 4 + 4\" y en ambos casos nos daría como resultado 12, para el caso de 2 · 2 al ser iguales los dos factores el producto sería \"2 + 2\" que es igual a 4 </s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset[\"text\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "496b1583-3fce-488a-8a40-11ecfc7bc93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dba4634e879408785a4ebeaf35fadf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_4bit=True,\n",
    "        #quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False # silence the warnings\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e28419-6910-48db-9297-44456b231839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80202a7f-9b0d-40ef-8025-88b342a8bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38ffe4cc-1587-42f7-9939-b9a815797a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07e2a5db-3de6-4925-bd60-8ff5b72ec30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72508c39-63fe-4091-bb91-c8f58a435d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 31:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.557300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.525500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-25 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-75 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-125 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-175 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-225 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./results/checkpoint-250 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.3375397033691405, metrics={'train_runtime': 1925.2221, 'train_samples_per_second': 0.519, 'train_steps_per_second': 0.13, 'total_flos': 1.7289112257921024e+16, 'train_loss': 1.3375397033691405, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "764d1cc3-9265-4e56-919d-3e8243b7a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Give me the python code for fine tuning a llm? [/INST] Here is a Python code for fine-tuning a language model:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "from torch.utils.tensorboard import SummaryWriter\n",
      "\n",
      "# Load the pre-trained model\n",
      "model = torch.hub.load(\"llm-pretrained-v2\", map_location=torch.device(\"cuda\"))\n",
      "\n",
      "# Define the dataset\n",
      "class TextDataset(Dataset):\n",
      "    def __init__(self, text):\n",
      "        self.text = text\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.text)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.text[index]\n",
      "CPU times: user 56.3 s, sys: 7.89 s, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"Give me the python code for fine tuning a llm?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb5404e8-8c11-41a6-b51a-75ac8d788369",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4589744-29c9-4899-9c30-cc940209ef64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524888a6-60d3-4376-9aa2-27f9661651c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a09635-c2bc-4938-9ea4-ab135a798865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
