{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM SFT using TRL - Transformer Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. imports\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "\n",
    "# 1. load a pretrained model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. initialize trainer\n",
    "ppo_config = {\"mini_batch_size\": 1, \"batch_size\": 1}\n",
    "config = PPOConfig(**ppo_config)\n",
    "ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n",
    "\n",
    "# 3. encode a query\n",
    "query_txt = \"This morning I went to the \"\n",
    "query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").to(model.pretrained_model.device)\n",
    "\n",
    "# 4. generate model response\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 20,\n",
    "}\n",
    "response_tensor = ppo_trainer.generate([item for item in query_tensor], return_prompt=False, **generation_kwargs)\n",
    "response_txt = tokenizer.decode(response_tensor[0])\n",
    "\n",
    "# 5. define a reward for response\n",
    "# (this could be any reward such as human feedback or output from another model)\n",
    "reward = [torch.tensor(1.0, device=model.pretrained_model.device)]\n",
    "\n",
    "# 6. train model with ppo\n",
    "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c638b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .. Let's assume we have a trained model using `PPOTrainer` and `AutoModelForCausalLMWithValueHead`\n",
    "\n",
    "# push the model on the Hub\n",
    "model.push_to_hub(\"my-fine-tuned-model-ppo\")\n",
    "\n",
    "# or save it locally\n",
    "model.save_pretrained(\"my-fine-tuned-model-ppo\")\n",
    "\n",
    "# load the model from the Hub\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"my-fine-tuned-model-ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016baee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd31b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6888875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample text data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This movie is great!\",\n",
    "        \"I didn't like the ending.\",\n",
    "        \"The acting was fantastic.\",\n",
    "        \"The plot was confusing.\",\n",
    "        \"It's a masterpiece.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the text data\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Function to annotate sentiment polarity\n",
    "def annotate_sentiment(text):\n",
    "    # Perform sentiment analysis or manual annotation\n",
    "    # Here, we simply assign 'positive' or 'negative' based on keywords\n",
    "    if 'great' in text or 'fantastic' in text or 'masterpiece' in text:\n",
    "        return 'positive'\n",
    "    elif 'didn\\'t like' in text or 'confusing' in text:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Annotate sentiment for each text in the DataFrame\n",
    "df['sentiment'] = df['text'].apply(annotate_sentiment)\n",
    "\n",
    "# Print the annotated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample python code for synthetic data generation using  Reinforcement Learning Augmented Intelligent Fabrication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.data_distribution = [0.8, 0.2]  # Class distribution for the synthetic data\n",
    "        self.observation_space = 2  # Dimensionality of the observations\n",
    "        self.action_space = 1  # Number of possible actions (binary classification)\n",
    "\n",
    "    def reset(self):\n",
    "        return np.random.randn(self.observation_space)  # Reset the environment to a new observation\n",
    "\n",
    "    def step(self, action):\n",
    "        # Generate a synthetic data point based on the action\n",
    "        if action == 0:  # Action 0 corresponds to class 0\n",
    "            observation = np.random.multivariate_normal([-1, -1], [[1, 0], [0, 1]])\n",
    "            reward = 1 if np.random.rand() < self.data_distribution[0] else 0\n",
    "        else:  # Action 1 corresponds to class 1\n",
    "            observation = np.random.multivariate_normal([1, 1], [[1, 0], [0, 1]])\n",
    "            reward = 1 if np.random.rand() < self.data_distribution[1] else 0\n",
    "        return observation, reward\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, exploration_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.q_table = np.zeros((2, 2))  # Q-table: state-action values\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.choice([0, 1])  # Random action (exploration)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[np.argmax(observation)])  # Greedy action selection\n",
    "\n",
    "    def update_q_table(self, observation, action, reward, next_observation):\n",
    "        best_next_action = np.argmax(self.q_table[np.argmax(next_observation)])\n",
    "        td_target = reward + self.discount_factor * self.q_table[np.argmax(next_observation)][best_next_action]\n",
    "        td_error = td_target - self.q_table[np.argmax(observation)][action]\n",
    "        self.q_table[np.argmax(observation)][action] += self.learning_rate * td_error\n",
    "\n",
    "# Initialize the environment and agent\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    for _ in range(100):  # Limiting the number of steps per episode\n",
    "        action = agent.choose_action(observation)\n",
    "        next_observation, reward = env.step(action)\n",
    "        agent.update_q_table(observation, action, reward, next_observation)\n",
    "        observation = next_observation\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = []\n",
    "for _ in range(1000):\n",
    "    observation = env.reset()\n",
    "    action = agent.choose_action(observation)\n",
    "    synthetic_data.append((observation, action))\n",
    "\n",
    "# Example usage of synthetic data\n",
    "for data_point in synthetic_data[:10]:\n",
    "    print(\"Observation:\", data_point[0], \"Action:\", data_point[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f267dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b3042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
